# -*- coding: utf-8 -*-
import os
#remove tensorflow warnings 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 

"""DataCleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CR8nzoGX3MAjoqEyIeDtky99nfuRdnIz

# Data Cleaning Code 
This is a code to clean data from a protein sequences dataset. The goal is to isolate only aminoacids sequences and their family

## First things firt
Let's open our dataset located in google drive using pandas dataframe. After that, the first 5 rows will be showed right bellow.
"""

# from google.colab import drive   #drive  - biblioteca p/ abrir arquivos do drive
import pandas as pd

#connect to google drive and open the dataframe using pandas 
# drive.mount('/content/drive') 
# seq_df = pd.read_csv('/content/drive/My Drive/BioinformaticsProject/sequence_data.csv')
# pdb_df = pd.read_csv('/content/drive/My Drive/BioinformaticsProject/pdb_data.csv')

seq_df = pd.read_csv('Data/sequence_data.csv')
pdb_df = pd.read_csv('Data/pdb_data.csv')

#print the 5 first rows of our datasets
seq_df.head()
pdb_df.head()

"""## Understanding the dataset
This dataset has a lot of gene sequences. The sequences can be DNA, RNA or aminoacids sequences. For our study we are only interested in aminoacids sequences, so we need to filter those.
"""

#select only proteins
protein_seq = seq_df[seq_df.macromoleculeType == 'Protein']
protein_class = pdb_df[pdb_df.macromoleculeType == 'Protein']

#filter the columns to only rest structureId, classification and sequence left
protein_class = protein_class[['structureId', 'classification']]
protein_seq   = protein_seq[['structureId', 'sequence']] 

#join using the structureId
data = protein_seq.join(protein_class.set_index('structureId'), on='structureId')

print('Total entries in our dataset: %i' %(len(data)))

#removing Nan entries
print('Removing Nan entries...')
data = data.dropna()

print('Total entries in our dataset: %i' %(len(data)))
data.head()

import numpy as np 
# Counts how many functions of protein ar available
counts = data.classification.value_counts()
print("Total protein functions: ", len(counts))

# Get classification types where counts are over 1000
types = np.asarray(counts[(counts > 10000)].index)
print("Total protein functions with more than 1000 samples: ", len(types))

# Filter dataset's records for classification types selected before
data = data[data.classification.isin(types)]
print("Total samples with more than 10000 samples: ", data.shape)

# Filter the dataset to get only sequences records with less than 500 aminoacids
data = data[(data.sequence.str.len() <= 500)]
print("Total samples with less than 500 aminoacids:", data.shape)

# Print total of every family entries
print(data['classification'].value_counts())

# Plot the total of samples of each family
# data['classification'].value_counts().plot.bar()

# Let's filter our data, to every protein family have the same total number of samples
max_size = len(data.loc[data['classification'] == 'LYASE'])
families = data['classification'].unique()
df = pd.DataFrame()
for family_name in families:
  df = df.append((data.loc[data['classification'] == family_name]).iloc[0:max_size])

# Print total entries of each family of out new dataframe
print("\nDataframe distribution after normalization")
print(df['classification'].value_counts())

data = df

"""## Pre-processing the selected data
For our model work well with our data, we first need to convert the sequences information to a numerical type of data. This is necessary because the neural network algorithm need numerical data to work, this is why we will do some transformations.

### Pre-process Labels
"""

from sklearn.preprocessing import LabelBinarizer

# Pre-process the labels - transform them in a one hot vector
lb = LabelBinarizer()
Y = lb.fit_transform(data['classification'])

"""### Pre-process Protein Sequences"""

from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split

# Set a maximum length of sequence, everything afterwards is discarded!
max_length = 256

# Create and fit a tokenizer
tokenizer = Tokenizer(char_level=True)
tokenizer.fit_on_texts(data['sequence'])

# Transform input data as word rank number sequences
X = tokenizer.texts_to_sequences(data['sequence'])
X = sequence.pad_sequences(X, maxlen=max_length)

# Split our processed dataset in train and test
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 12)

#print shape o training and testing dataset
print("X_train.shape: ",X_train.shape)
print("X_test.shape : ",X_test.shape)

print("y_train.shape: ", y_train.shape)
print("y_test.shape: ", y_test.shape)

# Define some important parameters to use later
max_tokens = len(tokenizer.word_index)  #gets how many token we have generated
print("Max tokens: ", max_tokens)

with open('X_train.npy', 'wb') as f:
	np.save(f, X_train)


with open('y_train.npy', 'wb') as f:
	np.save(f, y_train)

with open('X_test.npy', 'wb') as f:
	np.save(f, X_test)

with open('y_test.npy', 'wb') as f:
	np.save(f, y_test)

"""## Machine Learning Model

### Building a model with Keras
"""

# import tensorflow as tf
# from tensorflow.keras import layers

# embedding_dim = max_length    #holds max length of our sequencies
# max_features = max_tokens     #holds how many tokens were created during the feature extraction
# protein_functions = y_train.shape[1] #holds how many labels are available aka how many functions we are classifing

# # A integer input for vocab indices.
# inputs = tf.keras.Input(shape=(None,), dtype="int64")

# # Next, we add a layer to map those vocab indices into a space of dimensionality
# # 'embedding_dim'.
# x = layers.Embedding(max_features, embedding_dim)(inputs)
# x = layers.Dropout(0.5)(x)

# # Conv1D + global max pooling
# x = layers.Conv1D(128, 7, padding="valid", activation="relu", strides=3)(x)
# x = layers.Conv1D(128, 7, padding="valid", activation="relu", strides=3)(x)
# x = layers.GlobalMaxPooling1D()(x)

# # We add a vanilla hidden layer:
# x = layers.Dense(128, activation="relu")(x)
# x = layers.Dropout(0.5)(x)

# # We project onto a single unit output layer, and squash it with a sigmoid:
# predictions = layers.Dense(43, activation="sigmoid", name="predictions")(x)

# model = tf.keras.Model(inputs, predictions)

# # Compile the model with categorical crossentropy loss and an adam optimizer.
# model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

# """### Training Keras Model"""

# epochs = 100

# # Fit the model using the train dataset and save the accuracy metric through epochs
# history = model.fit(X_train, y_train, epochs= epochs, verbose=2, validation_split=0.2, shuffle=True)
# model.save("trained_model100.h5")

# """### Evaluating our Model"""

# from sklearn.metrics import classification_report, confusion_matrix

# #evaluate the model
# test_loss, test_accuracy = model.evaluate(X_test)
# print("Test Metrics")
# print("loss: %.4f  - accuracy: %.4f" %(test_loss, test_accuracy))

# y_prob = model.predict(X_test, y_test) 
# y_pred = y_prob.argmax(axis=-1)

# print(classification_report(y_test, y_pred))


# import seaborn as sn
# import numpy as np 
# import matplotlib.pyplot as plt

# #create a confusion matrix
# cm = confusion_matrix(y_test, y_pred)
# print(cm)

# #generate a heatmap normalized to our confusion matrix 
# sn.heatmap(cm/np.sum(cm), annot=True, cmap='Oranges')

# #set some last configurations to a nice plot :)
# plt.title("Confusion Matrix")
# plt.xlabel("Predicted labels")
# plt.ylabel("True labels")
# plt.savefig("cm100.png")

# #cleans our plot
# plt.clf()

# #Lets build a plot to our accuracy and loss during train/validation
# plt.plot(history.history['accuracy'], label='train acc')
# plt.plot(history.history['val_accuracy'], label='val acc')
# plt.plot(history.history['loss'], label='train loss')
# plt.plot(history.history['val_loss'], label='val loss')

# plt.title('Training Loss and Accuracy')
# plt.xlabel('epoch')
# plt.ylabel('Loss/Accuracy')

# plt.legend(loc='upper left')

# # plt.legend(['train', 'validation'])
# plt.savefig("loss-acc100.png")


# ssh -L 3000:litorina:<porta-rails> login@macalan.c3sl.ufpr.br

